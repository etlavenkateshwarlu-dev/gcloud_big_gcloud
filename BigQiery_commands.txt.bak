BigQuery 
-------
VM instance command 

> bq ls

--> create datset 
bq mk -d vctbatchnew45:cust_data_set1
-> see the dataset
bq ls vctbatchnew45:

-->to creat table 
bq mk -t vctbatchnew45:cust_data_set1.cust_temp1

bq mk -t vctbatchnew45:cust_data_set1.cust_tmpe2 cust_id:integer,cust_name:string,cust_loc:string




--> to see the schema

bq show vctbatchnew45:cust_data_set1.cust_tmpe2

--> to load data into table 
bq load --source_format=CSV --skip_leading_rows=1 vctbatchnew45:cust_data_set1.cust_tmpe2 gs://vctbatch45-dev-custdata-bucket/cust_data/cust_temp1.csv

--> to get the data from table 
bq query "select * from vctbatchnew45:cust_data_set1.cust_tmpe2"

gsutil cp *.csv gs://vctbatchdev45-dev-custdata-bucket/landing_zone

gs://vctbatchdev45-dev-custdata-bucket/landing_zone/customer_lo.csv
gs://vctbatchdev45-dev-custdata-bucket/landing_zone/customer_ny.csv
gs://vctbatchdev45-dev-custdata-bucket/landing_zone/orders.csv
gs://vctbatchdev45-dev-custdata-bucket/landing_zone/salesman_lo.csv
gs://vctbatchdev45-dev-custdata-bucket/landing_zone/salesman_ny.csv

create datset 
-------------
bq mk -d vctbatchnew45:cust_data_dev
bq mk -d vctbatchnew45:sales_data_dev
bq mk -d vctbatchnew45:orders_data_dev



cust_data_dev  dataset 
---------------------
bq load --source_format=CSV --skip_leading_rows=1 --autodetect \
vctbatchnew45:cust_data_dev.customer_lo gs://vctbatch45-dev-custdata-bucket/landing_zone/customer_lo.csv

bq load --source_format=CSV --skip_leading_rows=1 --autodetect \
vctbatchnew45:cust_data_dev.customer_ny gs://vctbatch45-dev-custdata-bucket/landing_zone/customer_ny.csv

sales_data_dev dataset
----------------------
bq load --source_format=CSV --skip_leading_rows=1 --autodetect \
vctbatchnew45:sales_data_dev.sales_lo gs://vctbatch45-dev-custdata-bucket/landing_zone/salesman_lo.csv

bq load --source_format=CSV --skip_leading_rows=1 --autodetect \
vctbatchnew45:sales_data_dev.sales_ny gs://vctbatch45-dev-custdata-bucket/landing_zone/salesman_ny.csv


orders_data_dev dataset
-----------------------
bq load --source_format=CSV --skip_leading_rows=1 --autodetect \
vctbatchnew45:orders_data_dev.orders gs://vctbatch45-dev-custdata-bucket/landing_zone/orders.csv

Quires to fetch data 
---------------------
bq query --use_legacy_sql=False "select * from  vctbatchnew45.cust_data_dev.customer_lo"

bq query --use_legacy_sql=False "select * from vctbatchnew45.cust_data_dev.customer_ny"

bq query --use_legacy_sql=False "select * from vctbatchnew45.sales_data_dev.sales_lo"

bq query --use_legacy_sql=False "select * from vctbatchnew45.sales_data_dev.sales_ny"

bq query --use_legacy_sql=False "select * from vctbatchnew45.orders_data_dev.orders"



-->bq mk -d vctbatchnew45:curated_cust_sal_order_dev


SELECT * FROM vctbatchnew45.cust_data_dev.customer_lo LIMIT 1000;

SELECT * FROM `vctbatchnew45.cust_data_dev.customer_ny` LIMIT 1000;

SELECT * FROM `vctbatchnew45.sales_data_dev.sales_lo` LIMIT 1000;

SELECT * FROM `vctbatchnew45.sales_data_dev.sales_ny` LIMIT 1000;

SELECT * FROM `vctbatchnew45.orders_data_dev.orders` LIMIT 1000;

insert into vctbatchnew45:stg_sales_dev.salesman_city_sales salesman_name(
salesman_name,
city,
total_sales
)
with cust_cte as (
select cust_name,city,salesman_id from vctbatchnew45.cust_data_dev.customer_lo 
union all 
select cust_name,city,salesman_id from vctbatchnew45.cust_data_dev.customer_ny
)
,
sales_cte as(
SELECT salesman_id,name,city FROM `vctbatchnew45.sales_data_dev.sales_lo` 
union all 
SELECT salesman_id,name,city FROM `vctbatchnew45.sales_data_dev.sales_ny`)
,
sales_order as (
select s.name,o.salesman_id,sum(o.purch_amt) as total_purchace_amount from sales_cte as s join `vctbatchnew45.orders_data_dev.orders` as o on s.salesman_id=o.salesman_id 
group by  o.salesman_id ,s.name
)
select so.name,c.city,so.total_purchace_amount from cust_cte as c join sales_order so on c.salesman_id=so.salesman_id 


INSERT your corrected result into the SILVER layer in BigQuery
==========
bq mk -d vctbatchnew45:stg_sales_dev
bq mk -t vctbatchnew45:stg_sales_dev.salesman_city_sales salesman_name:string,city:string,total_sales:float



expoer staging data to bicket 
--------------------------
gcloud storage buckets create gs://vctbatchnew45-staging-data-bucket


bq extract --destination_format=CSV --print_header=true vctbatchnew45:stg_sales_dev.salesman_city_sales  gs://vctbatchnew45-staging-data-bucket/stg_export/salesman_city_sales.csv


gsutil cat gs://vctbatchnew45-staging-data-bucket/stg_export/salesman_city_sales.csv


===Using schema===

schema.json 
-----------
[
  {"name":"cust_id","type": "INT64","mode":"NULLABLE"},
  {"name":"cust_name","type": "STRING","mode":"NULLABLE"},
  {"name":"city","type": "STRING","mode":"NULLABLE"},
  {"name":"grade","type": "INT64","mode":"NULLABLE"},
  {"name":"saleman_id","type": "INT64","mode":"NULLABLE"}
]


bq load --source_format=CSV \
--schema=schema.json \
vctbatch-dev45:Test_data_Set.cust_new \
gs://vctbatchdev45-dev-custdata-bucket/landing_zone/customer_lo_no_header.csv

bq  query --use_legacy_sql=False  "select * from vctbatch-dev45.Test_data_Set.cust_new"

--> read/refer schema form gcs bucket

bq load --source_format=CSV --schema=gs://vctbatchdev45-dev-custdata-bucket/schemas/schema.json vctbatch-dev45:Test_data_Set.cust_new_2

gs://vctbatchdev45-dev-custdata-bucket/landing_zone/customer_lo_no_header.csv



BigQuery can read schema from GCS in external tables
BigQuery can read data from GCS in loads
But cannot read schema from GCS in bq load

bq mk \
--external_table_definition=\
CSV=gs://vctbatchdev45-dev-custdata-bucket/landing_zone/customer_lo_no_header.csv \
--schema=gs://vctbatchdev45-dev-custdata-bucket/schemas/schema.json \
vctbatch-dev45:Test_data_Set.cust_new_1 

Note workinhg  -->BigQuery error in mk operation: Cannot load schema files from GCS.

external tble creation :
------------------------
bq mk --schema=schema.json \
--external_table_definition=CSV=gs://vctbatchdev45-dev-custdata-bucket/landing_zone/customer_lo_no_header.csv \
vctbatch-dev45:Test_data_Set.cust_new_external 


parquet file load 
-----------------

bq load --source_format=PARQUET \
vctbatch-dev45:Test_data_Set.cust_pq_data \
gs://vctbatchdev45-dev-custdata-bucket/landing_zone/parquet/userdata.parquet

bq show vctbatch-dev45:Test_data_Set.cust_pq_data
bq query --use_legacy_sql=False "select * from vctbatch-dev45.Test_data_Set.cust_pq_data"




bq load --source_format=CSV \
--schema=gs://vctbatchdev45-dev-custdata-bucket/schemas/schema.json \
vctbatch-dev45:Test_data_Set.cust__data1_new \
gs://vctbatchdev45-dev-custdata-bucket/landing_zone/customer_lo_no_header.csv 


========================External And Internal Table ================
step1 : using big query console
create table vctbatch-dev45.Test_data_Set.emp_tab (
eid integer,fname string ,lname string ,phone_number string,hire_date date,job_id string ,salary integer,dept_id string,country string
)

-->insert /load data into table using BigQuery console 
Load data into vctbatch-dev45.Test_data_Set.emp_tab
from files (
format='CSV',
uris=['gs://vctbatch45-dev-custdata-bucket-new/landing_zone/employee_1.csv']
);


Using BQ CLoud SDK 
---------------------
bq mk -t vctbatch-dev45:Test_data_Set.emp_tab1 \
eid:integer,fname:string,lname:string,phone_number:string,hire_date:date,job_id:string,salary:integer,dept_id:string,country:string 

bq load --source_format=CSV --skip_leading_rows=1 \
vctbatch-dev45:Test_data_Set.emp_tab1 \
gs://vctbatch45-dev-custdata-bucket-new/landing_zone/employee_1.csv

bq query --use_legacy_sql=False "select * from vctbatch-dev45.Test_data_Set.emp_tab1


external table creation 
--------------------
using big queey console
 
create external table vctbatch-dev45.Test_data_Set.emp_external_tab (
eid integer,fname string ,lname string ,phone_number string,hire_date date,job_id string ,salary integer,dept_id string,country string
)
options(
format='CSV',
uris=['gs://vctbatch45-dev-custdata-bucket-new/landing_zone/employee_1.csv'],
skip_leading_rows=1
);

select * from vctbatch-dev45.Test_data_Set.emp_external_tab

using BQ console aproach  using query 
---------------------------
--> we are not giving colums and data type defaultly it will take 
create external table vctbatch-dev45.Test_data_Set.emp_external_tab1
options(
format='CSV',
uris=['gs://vctbatch45-dev-custdata-bucket-new/landing_zone/employee_1.csv'],
skip_leading_rows=1
);

using cloud SDK 
---------------
bq mkdef --source_format=CSV --autodetect\
gs://vctbatch45-dev-custdata-bucket-new/landing_zone/employee_1.csv


